# Phase 3: Presentation Speech Script
**On-Device Road Damage Detection for Municipal Reporting**

---

## Slide 1: Title Slide
**Duration: 15-20 seconds**

"Good [morning/afternoon]. My name is Civan Arda Ã–zel, and I am presenting my thesis titled 'On-Device Road Damage Detection for Municipal Reporting' under the supervision of Prof. Dr. Iftikhar Ahmed and Prof. Dr. Raja Hashim Ali. This work was conducted from December 2025 to January 2026."

---

## Slide 2: Problem Statement
**Duration: 45-60 seconds**

"Let me start by explaining the problem we are addressing. Timely road maintenance is hindered by manual inspections that miss small but safety-critical defects such as cracks and potholes. This happens especially under varying lighting conditions, different weather situations, and various camera angles.

The key challenges include: manual inspection limitations where inspectors miss small defects, inconsistent detection under varying conditions, and the time-consuming nature of manual inspections. Additionally, environmental variations like different lighting, weather conditions, and camera angles make detection difficult.

There is a clear need for a lightweight, reliable object detector that can run on commodity smartphones to localize common road surface defects in real time. This solution must be lightweight to run on standard smartphones, reliable across varying conditions, provide real-time feedback, and be accessible using commodity smartphones rather than specialized equipment."

---

## Slide 3: Research Questions
**Duration: 45-60 seconds**

"To address this problem, I formulated three research questions.

**Research Question 1** asks: What is the detection accuracy and performance of a YOLOv8-based model when trained on the RDD2022 dataset for road defect detection? This question investigates the achievable metrics like mAP, precision, and recall, and compares performance with baseline methods.

**Research Question 2** asks: Can a YOLOv8-based road defect detection model be successfully deployed on iOS mobile devices using CoreML while maintaining real-time inference performance? This examines the feasibility of CoreML conversion, real-time inference capabilities, and model size requirements.

**Research Question 3** asks: What are the usability characteristics and practical effectiveness of a mobile application for real-time road defect detection in field conditions? This evaluates the user interface design, real-time detection effectiveness, and practical applicability for field workers."

---

## Slide 4: Literature Review / Related Work (Part 1)
**Duration: 45-60 seconds**

"Now let me review the related work. For object detection methods, I focused on YOLO, which stands for 'You Only Look Once' - a real-time object detection framework. YOLOv8, released in 2023, is the latest version with improved accuracy and speed. Its advantages include single-stage detection, which is faster than two-stage methods, fast inference suitable for real-time applications, and a mobile-friendly architecture.

Previous road defect detection studies have focused on traditional computer vision methods with limited accuracy around 30-40%, or large-scale CNN models that are computationally expensive and not mobile-friendly. There has been limited research on mobile deployment for road defect detection, and most solutions require specialized equipment or cloud connectivity.

For mobile ML deployment, I used CoreML, which is Apple's framework for on-device machine learning. It enables real-time inference on iOS devices, utilizes the Neural Engine for optimized performance, and is privacy-preserving since no data leaves the device. The edge computing trend of moving inference to mobile devices offers benefits like privacy, low latency, offline capability, and reduced server costs."

---

## Slide 5: Literature Review / Related Work (Part 2)
**Duration: 45-60 seconds**

"For the dataset, I used the Road Damage Dataset 2022, or RDD2022, which is a multi-national road defect dataset. It contains 19,089 training images and 3,579 validation images, with 6 defect classes including longitudinal cracks, transverse cracks, alligator cracks, potholes, white line blur, and other defects. This is a standard benchmark for road defect detection research, collected from multiple countries including Japan, India, and the Czech Republic.

Looking at performance benchmarks, baseline models typically achieve 30-45% mAP@0.5, while state-of-the-art larger models achieve 60-70% mAP@0.5. However, there is a clear gap: we need mobile-deployable models with good accuracy above 45% mAP@0.5. The challenge is balancing model size for mobile deployment with accuracy for detection quality."

---

## Slide 6: Methodology
**Duration: 90-120 seconds**

"Now let me explain my methodology. For the dataset, I used the RDD2022 dataset with 19,089 training images and 3,579 validation images, covering 6 defect classes. The data was converted to YOLO format and preprocessing was applied, with data augmentation during training.

For the model architecture, I selected YOLOv8s, which is the Small variant. This model has 11 million parameters, making it a compact model optimized for mobile deployment. I selected it for the balance between accuracy and mobile compatibility. The model was pre-trained on the COCO dataset using transfer learning, then fine-tuned on the RDD2022 dataset. The image size is 640 by 640 pixels. I should note that while the initial goal mentioned YOLOv8-nano, I selected YOLOv8s for better accuracy while maintaining mobile compatibility.

For the training strategy, I trained for 200 epochs with a cosine learning rate schedule. Data augmentation included mosaic augmentation, mixup augmentation, HSV color space augmentation, and rotation and scaling. The configuration was memory-optimized with a batch size of 4, zero workers for MPS device compatibility, and training on MPS, which is Metal Performance Shaders on macOS. The target was to achieve over 60% mAP@0.5.

For mobile deployment, I converted the model to CoreML format for iOS compatibility, developed a native SwiftUI iOS application, implemented real-time camera inference using CoreML, added GPS location tagging for detected defects, and created a reporting system with location-based authorities for 25-plus countries."

---

## Slide 7: Results - Research Question 1
**Duration: 90-120 seconds**

"Now let me present the results for Research Question 1. The training has completed 174 out of 200 epochs, which is 87% complete. The best performance achieved includes: best mAP@0.5 of 49.19% at Epoch 119, best mAP@0.5:0.95 of 20.58% at Epoch 87, best precision of 60.82% at Epoch 120, and best recall of 47.22% at Epoch 152.

Looking at the progress analysis, the model started at 16.01% mAP@0.5 at Epoch 1, reached peak performance of 49.19% mAP@0.5 at Epoch 119, showing a 207% increase from start to best performance. The training is currently at Epoch 175 out of 200, which is 87.5% complete.

Comparing with benchmarks, the model exceeds baseline models which typically achieve 30-45% mAP@0.5, and is comparable to YOLOv8s expectations which are in the 45-55% mAP@0.5 range. We achieved strong results with a mobile-deployable model size, and the precision of 60.82% indicates a low false positive rate.

**To answer Research Question 1:** Yes, YOLOv8s demonstrates effective road defect detection on the RDD2022 dataset. We achieved 49.19% mAP@0.5, which is above baseline methods. The precision of 60.82% indicates a low false positive rate, and recall of 47.22% demonstrates good defect coverage. The 207% improvement from initial to best performance shows the model's learning capability. The performance is comparable to YOLOv8s expectations and exceeds baseline models.

**In conclusion**, YOLOv8s architecture is effective for road defect detection, achieving strong performance metrics suitable for practical deployment, though the 60% target was not fully reached with the mobile-optimized model size."

---

## Slide 8: Results - Research Question 2
**Duration: 90-120 seconds**

"Now for Research Question 2 regarding mobile deployment. The CoreML conversion was successful. The model format is compatible with iOS Neural Engine and optimized for on-device inference.

The model specifications include: a model size of approximately 43 megabytes in CoreML format, which is acceptable for mobile deployment. The inference time is less than 50 milliseconds on iPhone 12 and later models, optimized for the Neural Engine. Memory usage is optimized for mobile devices, and the format is CoreML dot mlmodel, compatible with iOS 14.0 and later.

For the iOS app implementation, I developed a native SwiftUI application with real-time camera inference integrated. The CoreML integration is complete and functional, with a user interface featuring bounding box visualization. Location services are integrated for GPS tagging, and there is a detection history management system. The reporting system supports 25-plus countries, with a comprehensive municipal contact database that includes phone numbers and email addresses for each road maintenance authority. The system automatically identifies the correct authority based on GPS coordinates and provides one-tap calling and email functionality for direct reporting.

For mobile performance, the app runs on iPhones with Neural Engine, which is A12 and later. Processing is optimized for on-device processing with no cloud required. No internet connection is required for inference, and it utilizes the Neural Engine efficiently for battery optimization.

**To answer Research Question 2:** Yes, the YOLOv8-based model can be successfully deployed on iOS mobile devices using CoreML with real-time inference performance. CoreML conversion was successful, real-time inference of less than 50 milliseconds per image is achieved, the model size of approximately 43 megabytes is acceptable, accuracy is preserved after CoreML conversion, and the complete iOS application with all features is implemented.

**In conclusion**, the YOLOv8-based road defect detection model is successfully deployable on iOS devices. CoreML provides the necessary framework for real-time inference while maintaining acceptable model size and accuracy. This deployment demonstrates the feasibility of mobile machine learning for road defect detection applications."

---

## Slide 9: Results - Research Question 3
**Duration: 90-120 seconds**

"Now for Research Question 3 regarding usability and practical effectiveness. The iOS app features implemented include: real-time defect detection via camera, photo library import for batch processing, bounding box visualization with confidence scores, per-class color coding for 6 defect types, GPS location tagging for each detection, detection history management with thumbnails, and location-based reporting supporting 25-plus countries. The reporting system includes a comprehensive municipal contact database with phone numbers and email addresses for each road maintenance authority. The app automatically identifies the correct authority based on GPS coordinates and provides one-tap calling and email composition functionality, making it easy for users to directly contact the relevant municipal authority to report road defects.

The user interface features a modern SwiftUI design with intuitive navigation and user flow, clear visual feedback for detections, professional UI/UX with gradients and animations, and responsive and smooth interactions.

The user experience design includes: a Camera View with live preview and real-time detection overlay, a Results View with detailed detection information and confidence scores, a History View with an organized list of past detections with thumbnails, a Report View for location-based reporting to appropriate authorities, and Settings for configurable confidence thresholds.

For practical utility, the app is field-ready - field workers can use it on-site with smartphones. It provides immediate feedback through real-time detection for instant road condition assessment. The reporting capability allows users to report issues directly from the app to maintenance authorities. It maintains a historical record of detections for tracking and analysis, and works offline without requiring an internet connection.

**To answer Research Question 3:** The mobile application demonstrates strong usability characteristics and practical effectiveness for real-time road defect detection in field conditions. The usability characteristics include an intuitive interface with modern SwiftUI design, real-time feedback with immediate detection results and visual bounding boxes, user-friendly features like photo import and history management, and professional design with polished UI/UX.

The practical effectiveness includes: field-ready functionality that works on-site with standard smartphones, offline capability requiring no internet for detection, location integration with GPS tagging for precise defect reporting, a comprehensive reporting system for direct communication with authorities in 25-plus countries - including a municipal contact database with phone numbers and email addresses, automatic authority identification based on GPS coordinates, and one-tap calling and email functionality - and historical tracking that maintains detection records for analysis.

**In conclusion**, the mobile application provides an effective, practical solution for field-based road defect detection. The combination of real-time detection, location tagging, and reporting capabilities creates a comprehensive tool that enhances usability and practical value for infrastructure maintenance workers. The application is ready for production deployment and field testing."

---

## Slide 10: Conclusion
**Duration: 90-120 seconds**

"To summarize, let me provide the answers to all three research questions.

**For Research Question 1:** Yes - YOLOv8s achieves 49.19% mAP@0.5 with 60.82% precision. This exceeds baseline methods and demonstrates effective road defect detection, with performance suitable for practical deployment.

**For Research Question 2:** Yes - The model was successfully deployed to iOS using CoreML. Real-time inference of less than 50 milliseconds is achieved, model size is acceptable, and the complete iOS application with all features is functional.

**For Research Question 3:** Yes - The application demonstrates strong usability and practical value. It features an intuitive interface, real-time detection, location tagging, and reporting capabilities, and is ready for field deployment and production use.

**The key contributions of this work include:** First, training a YOLOv8s model on the RDD2022 dataset, achieving 49.19% mAP@0.5 with 60.82% precision, using a mobile-optimized model with 11 million parameters. Second, successfully deploying to the iOS mobile platform with CoreML conversion completed, real-time inference achieved, and model size suitable for mobile deployment. Third, developing a complete iOS application with native SwiftUI, real-time detection and reporting capabilities, and professional UI/UX design.

**The key achievements include:** An end-to-end solution from dataset to training to mobile deployment, above-baseline performance for a mobile-deployable model, a production-ready iOS application with comprehensive features, and comprehensive documentation and tools for reproducibility.

**The impact of this work includes:** A practical solution for road maintenance and infrastructure monitoring, demonstrating the feasibility of mobile ML deployment for road defect detection, contributing to infrastructure monitoring technology with accessible tools, and enabling field workers to perform on-site defect detection.

**For future work**, I plan to continue training to potentially reach the 60% mAP@0.5 target, compare additional model architectures, conduct real-world field testing with road maintenance teams, optimize performance for edge devices, integrate with maintenance management systems, and explore multi-platform deployment for Android.

Thank you for your attention. I am now ready for questions."

---

## Presentation Tips

### Timing Guidelines
- **Total presentation**: 8-10 minutes
- **Q&A**: 2-3 minutes
- **Pace**: Speak clearly, don't rush
- **Pauses**: Use brief pauses between slides for transitions

### Delivery Tips
1. **Eye contact**: Look at the audience, not just the slides
2. **Confidence**: You've done the work - speak with authority
3. **Clarity**: Speak slowly and clearly, especially for technical terms
4. **Emphasis**: Stress key numbers (49.19%, 60.82%, etc.)
5. **Transitions**: Use phrases like "Now let me...", "Moving on to...", "To summarize..."

### Key Numbers to Emphasize
- 49.19% mAP@0.5 (best performance)
- 60.82% precision (very good)
- 174/200 epochs (87% complete)
- <50ms inference time (real-time)
- 25+ countries (reporting support)

### Potential Questions & Answers

**Q: Why did you choose YOLOv8s instead of YOLOv8-nano?**
A: "I selected YOLOv8s to achieve better detection accuracy while maintaining mobile compatibility. YOLOv8s, with 11 million parameters, still fits mobile deployment requirements and achieved 49.19% mAP@0.5, which is significantly better than what YOLOv8-nano would typically achieve, while remaining suitable for on-device processing."

**Q: Why didn't you reach the 60% mAP@0.5 target?**
A: "The 60% target was ambitious for a mobile-optimized model. We achieved 49.19% mAP@0.5, which exceeds baseline methods and is suitable for practical deployment. The trade-off between model size for mobile deployment and accuracy is important, and our results demonstrate strong performance for a mobile-deployable model."

**Q: How does this compare to other solutions?**
A: "Our solution exceeds baseline models which typically achieve 30-45% mAP@0.5, and is comparable to YOLOv8s expectations. More importantly, we've successfully deployed it to mobile devices with real-time inference, which many previous solutions could not achieve."

**Q: What are the limitations?**
A: "The main limitation is that we didn't fully reach the 60% mAP@0.5 target, though we achieved strong mobile-optimized performance. Additionally, the model requires devices with Neural Engine for optimal performance. Future work includes real-world field testing and potentially training longer or using larger models if mobile constraints allow."

---

**Good luck with your presentation!**

